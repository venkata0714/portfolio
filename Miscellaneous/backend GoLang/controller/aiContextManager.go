package controllers

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"regexp"
	"sort"
	"strings"
	"sync"
	"time"

	"example.com/portfolio-backend/config"
	"go.mongodb.org/mongo-driver/bson"
)

// Category weighting and boosting factors
var categoryWeights = map[string]float64{"db": 0.7, "resume": 0.3, "github": 0.1}
var queryBoost = map[string]float64{"db": 0.1, "resume": 0.1, "github": 0.1}
var dbTerms = []string{"experience", "project", "honors", "skills", "involvement", "yearinreview"}
var resumeTerms = []string{"education", "experience", "skills", "projects", "honors", "involvement", "year in review"}

// In-memory caches for context snapshots
var contextMeta = struct {
	DbContextLastUpdate      string `bson:"dbContextLastUpdate,omitempty"`
	GithubContextLastUpdate  string `bson:"githubContextLastUpdate,omitempty"`
	ResumeContextLastUpdate  string `bson:"resumeContextLastUpdate,omitempty"`
}{}
var memoryIndexMeta = struct {
	LastUpdate string `bson:"lastUpdate,omitempty"`
}{}

// MemoryIndex and context data
type MemoryItem struct {
	Category string
	Text     string
	Embedding []float32
	Norm     float64
}
var memoryIndex []MemoryItem

// Initialize AI context: load context meta, ensure snapshots are up to date, build memory index.
func InitContext() error {
	ctx := context.Background()
	// Load context metadata (timestamps)
	loadContextMeta(ctx)
	loadMemoryIndexMeta(ctx)
	today := time.Now().UTC().Truncate(24 * time.Hour)
	// If any context snapshot is missing or not updated today, update it
	if contextMeta.DbContextLastUpdate == "" || parseTime(contextMeta.DbContextLastUpdate).Before(today) {
		if err := updateDbContextFile(ctx); err != nil {
			return fmt.Errorf("failed to update DB context: %w", err)
		}
	}
	if contextMeta.GithubContextLastUpdate == "" || parseTime(contextMeta.GithubContextLastUpdate).Before(today) {
		if err := updateGithubContextFile(ctx); err != nil {
			return fmt.Errorf("failed to update GitHub context: %w", err)
		}
	}
	if contextMeta.ResumeContextLastUpdate == "" || parseTime(contextMeta.ResumeContextLastUpdate).Before(today) {
		if err := updateResumeContextFile(ctx); err != nil {
			return fmt.Errorf("failed to update Resume context: %w", err)
		}
	}
	// Build memory index (do not force rebuild unless needed)
	if err := buildMemoryIndex(ctx, false); err != nil {
		return fmt.Errorf("failed to build memory index: %w", err)
	}
	// Schedule daily context updates and weekly memory index rebuild:
	go scheduleDailyTasks()
	return nil
}

func parseTime(iso string) time.Time {
	t, _ := time.Parse(time.RFC3339, iso)
	return t
}

// loadContextMeta loads the contextMeta document from the AI DB.
func loadContextMeta(ctx context.Context) {
	db := config.GetDBAI()
	var doc struct {
		DbContextLastUpdate     string `bson:"dbContextLastUpdate"`
		GithubContextLastUpdate string `bson:"githubContextLastUpdate"`
		ResumeContextLastUpdate string `bson:"resumeContextLastUpdate"`
	}
	_ = db.Collection("contextMeta").FindOne(ctx, bson.M{"_id": "contextMeta"}).Decode(&doc)
	if doc.DbContextLastUpdate != "" || doc.GithubContextLastUpdate != "" || doc.ResumeContextLastUpdate != "" {
		contextMeta.DbContextLastUpdate = doc.DbContextLastUpdate
		contextMeta.GithubContextLastUpdate = doc.GithubContextLastUpdate
		contextMeta.ResumeContextLastUpdate = doc.ResumeContextLastUpdate
	}
}

// saveContextMeta persists the contextMeta (with updated timestamps) to the DB.
func saveContextMeta(ctx context.Context) {
	db := config.GetDBAI()
	_, err := db.Collection("contextMeta").UpdateOne(ctx,
		bson.M{"_id": "contextMeta"},
		bson.M{"$set": contextMeta},
		optionsUpsert(),
	)
	if err != nil {
		log.Println("Error saving contextMeta:", err)
	}
}

// loadMemoryIndexMeta loads memoryIndexMeta document from DB.
func loadMemoryIndexMeta(ctx context.Context) {
	db := config.GetDBAI()
	var doc struct{ LastUpdate string `bson:"lastUpdate"` }
	_ = db.Collection("memoryIndexMeta").FindOne(ctx, bson.M{"_id": "memoryIndexMeta"}).Decode(&doc)
	memoryIndexMeta.LastUpdate = doc.LastUpdate
}

// saveMemoryIndexMeta persists the memoryIndexMeta to DB.
func saveMemoryIndexMeta(ctx context.Context) {
	db := config.GetDBAI()
	_, err := db.Collection("memoryIndexMeta").UpdateOne(ctx,
		bson.M{"_id": "memoryIndexMeta"},
		bson.M{"$set": bson.M{"lastUpdate": memoryIndexMeta.LastUpdate}},
		optionsUpsert(),
	)
	if err != nil {
		log.Println("Error saving memoryIndexMeta:", err)
	}
}

// Utility to get an upsert option
func optionsUpsert() *options.UpdateOptions {
	opts := options.Update()
	opts.SetUpsert(true)
	return opts
}

// updateDbContextFile aggregates all data collections and stores a snapshot in AI DB.
func updateDbContextFile(ctx context.Context) error {
	dbPrimary := config.GetDB()
	// Fetch data from each collection, excluding certain fields
	collections := []struct {
		Name      string
		Projection bson.M
	}{
		{"experienceTable", bson.M{"_id": 0, "experienceLink": 0, "experienceURLs": 0, "likesCount": 0, "experienceImages": 0}},
		{"honorsExperienceTable", bson.M{"_id": 0, "honorsExperienceLink": 0, "honorsExperienceURLs": 0, "likesCount": 0, "honorsExperienceImages": 0}},
		{"involvementTable", bson.M{"_id": 0, "involvementLink": 0, "involvementURLs": 0, "likesCount": 0, "involvementImages": 0}},
		{"projectTable", bson.M{"_id": 0, "projectLink": 0, "projectURLs": 0, "likesCount": 0, "projectImages": 0}},
		{"skillsCollection", bson.M{"_id": 0}},
		{"skillsTable", bson.M{"_id": 0}},
		{"yearInReviewTable", bson.M{"_id": 0, "yearInReviewLink": 0, "yearInReviewURLs": 0, "likesCount": 0, "yearInReviewImages": 0}},
	}
	aggregated := make(map[string]interface{})
	for _, col := range collections {
		// Query all docs with projection
		cur, err := dbPrimary.Collection(col.Name).Find(ctx, bson.M{"deleted": bson.M{"$ne": true}}, options.Find().SetProjection(col.Projection))
		if err != nil {
			return err
		}
		var docs []bson.M
		if err = cur.All(ctx, &docs); err != nil {
			return err
		}
		aggregated[col.Name] = docs
	}
	// Remove empty fields recursively
	cleaned := removeEmptyFields(aggregated)
	if cleaned == nil {
		cleaned = map[string]interface{}{} // ensure not nil
	}
	// Upsert snapshot into dbContexts collection
	dbAI := config.GetDBAI()
	_, err := dbAI.Collection("dbContexts").UpdateOne(ctx,
		bson.M{"_id": "current"},
		bson.M{"$set": bson.M{"data": cleaned, "createdAt": time.Now()}},
		optionsUpsert(),
	)
	if err != nil {
		return err
	}
	// Update contextMeta timestamp
	contextMeta.DbContextLastUpdate = time.Now().Format(time.RFC3339)
	saveContextMeta(ctx)
	log.Printf("✅ dbContexts snapshot saved (%d tables)", len(aggregated))
	return nil
}

// updateGithubContextFile fetches repo data from GitHub and stores snapshot in AI DB.
func updateGithubContextFile(ctx context.Context) error {
	// Fetch all repos via GitHub API
	repos, err := fetchAllRepos()
	if err != nil {
		return err
	}
	out := make([]map[string]interface{}, 0, len(repos))
	for _, r := range repos {
		info := map[string]interface{}{
			"name":        r.Name,
			"full_name":   r.FullName,
			"description": strings.TrimSpace(r.Description),
			"html_url":    r.HTMLURL,
			"language":    r.Language,
			"visibility":  ifThenElse(r.Private, "private", "public"),
			"created_at":  r.CreatedAt,
			"updated_at":  r.UpdatedAt,
			"pushed_at":   r.PushedAt,
			"stargazers_count": r.StargazersCount,
			"forks_count":      r.ForksCount,
		}
		// Try to fetch README content (raw text)
		md, err := fetchRepoReadme(r.FullName)
		if err == nil && md != "" {
			trimmed := strings.TrimSpace(md)
			if len(trimmed) > 3000 {
				trimmed = trimmed[:2995] + "..."
			}
			info["readme"] = trimmed
		} else if err != nil && err.Error() != "404" {
			log.Printf("README error %s: %v", r.FullName, err)
		}
		// Remove empty fields from info
		cleaned := removeEmptyFields(info)
		if cleaned != nil {
			out = append(out, cleaned.(map[string]interface{}))
		} else {
			out = append(out, map[string]interface{}{})
		}
	}
	// Upsert snapshot into githubContexts
	dbAI := config.GetDBAI()
	_, err := dbAI.Collection("githubContexts").UpdateOne(ctx,
		bson.M{"_id": "current"},
		bson.M{"$set": bson.M{"data": out, "createdAt": time.Now()}},
		optionsUpsert(),
	)
	if err != nil {
		return err
	}
	contextMeta.GithubContextLastUpdate = time.Now().Format(time.RFC3339)
	saveContextMeta(ctx)
	log.Printf("✅ githubContexts snapshot saved (%d repos)", len(out))
	return nil
}

// updateResumeContextFile parses the resume PDF text and stores snapshot in AI DB.
func updateResumeContextFile(ctx context.Context) error {
	resumeText := parseResumePDF()  // (Assume we have a function to read "data/Singh_Kartavya_Resume2025.pdf" and return its text)
	snapshot := map[string]string{"resume_text": strings.TrimSpace(resumeText)}
	dbAI := config.GetDBAI()
	_, err := dbAI.Collection("resumeContexts").UpdateOne(ctx,
		bson.M{"_id": "current"},
		bson.M{"$set": bson.M{"data": snapshot, "createdAt": time.Now()}},
		optionsUpsert(),
	)
	if err != nil {
		return err
	}
	contextMeta.ResumeContextLastUpdate = time.Now().Format(time.RFC3339)
	saveContextMeta(ctx)
	log.Printf("✅ resumeContexts snapshot saved (%d chars)", len(snapshot["resume_text"]))
	return nil
}

// removeEmptyFields recursively removes null/empty fields from objects/arrays.
func removeEmptyFields(value interface{}) interface{} {
	switch v := value.(type) {
	case []interface{}:
		var newArr []interface{}
		for _, elem := range v {
			cleaned := removeEmptyFields(elem)
			if cleaned != nil && cleaned != "" {
				// Only add if not nil/empty
				newArr = append(newArr, cleaned)
			}
		}
		if len(newArr) == 0 {
			return nil
		}
		return newArr
	case map[string]interface{}:
		for key, elem := range v {
			cleaned := removeEmptyFields(elem)
			if cleaned == nil || cleaned == "" {
				delete(v, key)
			} else {
				v[key] = cleaned
			}
		}
		if len(v) == 0 {
			return nil
		}
		return v
	case string:
		trimmed := strings.TrimSpace(v)
		if trimmed == "" {
			return nil
		}
		return trimmed
	default:
		// For numbers, bool, etc., return as is (unless nil)
		if v == nil {
			return nil
		}
		return v
	}
}

// loadAndChunkData gathers the latest DB, GitHub, and Resume context data and splits into chunks for embedding.
func loadAndChunkData(ctx context.Context) ([]MemoryItem, error) {
	// Get context JSON strings
	dbJSON, err := getDbContextFile(ctx)
	if err != nil {
		return nil, err
	}
	ghJSON, err := getGithubContextFile(ctx)
	if err != nil {
		return nil, err
	}
	resJSON, err := getResumeContextFile(ctx)
	if err != nil {
		return nil, err
	}
	var dbData map[string]interface{}
	var ghData []interface{}
	var resData map[string]string
	_ = json.Unmarshal([]byte(dbJSON), &dbData)
	_ = json.Unmarshal([]byte(ghJSON), &ghData)
	_ = json.Unmarshal([]byte(resJSON), &resData)

	// Create chunks
	chunks := []MemoryItem{}
	chunks = append(chunks, chunkDbContext(dbData)...)
	chunks = append(chunks, chunkGithubContext(ghData)...)
	if resText, ok := resData["resume_text"]; ok {
		chunks = append(chunks, chunkResumeContext(resText)...)
	}
	return chunks, nil
}

// chunkDbContext converts aggregated DB context data into labeled text chunks.
func chunkDbContext(dbData map[string]interface{}) []MemoryItem {
	var chunks []MemoryItem
	// Mapping table names to pretty labels
	tableLabels := map[string]string{
		"experienceTable":       "Experience",
		"honorsExperienceTable": "Honors Experience",
		"involvementTable":      "Involvement",
		"projectTable":          "Project",
		"skillsCollection":      "Skills Collection",
		"skillsTable":           "Skill",
		"yearInReviewTable":     "Year In Review",
	}
	for table, data := range dbData {
		label, ok := tableLabels[table]
		if !ok {
			label = table
		}
		// Each data should be a slice of docs
		if docs, ok := data.([]interface{}); ok {
			for _, doc := range docs {
				if m, ok := doc.(map[string]interface{}); ok {
					// Build chunk text: Start with label and possibly a title field
					var titleVal string
					var shortFields []string
					var longText string
					for k, val := range m {
						strVal := fmt.Sprintf("%v", val)
						if strings.HasSuffix(strings.ToLower(k), "title") || strings.HasSuffix(strings.ToLower(k), "name") {
							titleVal = strVal
						} else if len(strVal) > 100 || strings.Contains(strVal, "\n") || strings.Contains(strings.ToLower(k), "description") {
							// treat as long text
							longText += strVal
							if !strings.HasSuffix(longText, "\n") {
								longText += "\n"
							}
						} else {
							// short field (like date, location, etc.)
							if strVal != "" {
								if strings.Contains(strings.ToLower(k), "password") {
									// skip sensitive
									continue
								}
								shortFields = append(shortFields, fmt.Sprintf("%s: %s", k, strVal))
							}
						}
					}
					text := label
					if titleVal != "" {
						text += " - " + titleVal
					}
					if len(shortFields) > 0 {
						text += " (" + strings.Join(shortFields, "; ") + ")"
					}
					if longText != "" {
						lt := strings.TrimSpace(longText)
						text += "\n" + lt
					}
					chunks = append(chunks, MemoryItem{Category: "db", Text: text})
				}
			}
		}
	}
	return chunks
}

// chunkGithubContext converts GitHub repo list into text chunks.
func chunkGithubContext(ghData []interface{}) []MemoryItem {
	var chunks []MemoryItem
	for _, item := range ghData {
		if repo, ok := item.(map[string]interface{}); ok {
			var line string
			name := fmt.Sprintf("%v", repo["full_name"])
			language := fmt.Sprintf("%v", repo["language"])
			if language == "<nil>" {
				language = ""
			}
			description := fmt.Sprintf("%v", repo["description"])
			readme := fmt.Sprintf("%v", repo["readme"])
			line = "GitHub Repo - " + name
			if language != "" && language != "<nil>" {
				line += " (" + language + ")"
			}
			if description != "" && description != "<nil>" {
				line += "\nDescription: " + description
			}
			if readme != "" && readme != "<nil>" {
				line += "\nREADME: " + readme
			}
			chunks = append(chunks, MemoryItem{Category: "github", Text: strings.TrimSpace(line)})
		}
	}
	return chunks
}

// chunkResumeContext splits resume text into sections (chunks) based on known section headings.
func chunkResumeContext(resumeText string) []MemoryItem {
	resumeText = strings.TrimSpace(resumeText)
	if resumeText == "" {
		return nil
	}
	var chunks []MemoryItem
	// Use regex to find section headings
	pattern := `(?m)^(Education|Experience|Skills|Projects|Honors|Involvement|Year ?in ?Review)\b`
	re := regexp.MustCompile(pattern)
	indices := re.FindAllStringIndex(resumeText, -1)
	if len(indices) <= 1 {
		// No multiple sections found, treat entire resume as one chunk
		chunks = append(chunks, MemoryItem{Category: "resume", Text: resumeText})
	} else {
		// Multiple sections: split at those headings
		for i := 0; i < len(indices); i++ {
			startIdx := indices[i][0]
			var endIdx int
			if i < len(indices)-1 {
				endIdx = indices[i+1][0]
			} else {
				endIdx = len(resumeText)
			}
			sectionText := strings.TrimSpace(resumeText[startIdx:endIdx])
			if sectionText != "" {
				chunks = append(chunks, MemoryItem{Category: "resume", Text: sectionText})
			}
		}
	}
	return chunks
}

// buildMemoryIndex builds (or loads) the memory index of embedded chunks for retrieval.
func buildMemoryIndex(ctx context.Context, forceRebuild bool) error {
	now := time.Now()
	currentMonth := now.Year()*12 + int(now.Month())
	lastUpdateMonth := -1
	if memoryIndexMeta.LastUpdate != "" {
		t := parseTime(memoryIndexMeta.LastUpdate)
		lastUpdateMonth = t.Year()*12 + int(t.Month())
	}
	dbAI := config.GetDBAI()
	// Count current memoryIndex docs in DB
	count, _ := dbAI.Collection("memoryIndex").CountDocuments(ctx, bson.M{})
	if !forceRebuild && lastUpdateMonth == currentMonth && count > 0 {
		// Load memoryIndex from DB
		cur, err := dbAI.Collection("memoryIndex").Find(ctx, bson.M{})
		if err != nil {
			return err
		}
		var docs []struct {
			Category  string             `bson:"category"`
			Text      string             `bson:"text"`
			Embedding primitive.A        `bson:"embedding"`
		}
		if err = cur.All(ctx, &docs); err != nil {
			return err
		}
		memoryIndex = memoryIndex[:0]
		for _, doc := range docs {
			// Convert primitive.A (array of float64) to []float32
			vec := make([]float32, len(doc.Embedding))
			for i, v := range doc.Embedding {
				// Assuming all elements are float64
				if f, ok := v.(float64); ok {
					vec[i] = float32(f)
				}
			}
			// Compute norm
			var sum float64
			for _, x := range vec {
				sum += float64(x * x)
			}
			norm := 0.0
			if sum > 0 {
				norm = float64(fmt.Sprintf("%.6f", sum)) // just to restrict precision
				norm = sum // fix: actual sqrt for norm
			}
			norm = sqrt(sum) // conceptually
			memoryIndex = append(memoryIndex, MemoryItem{Category: doc.Category, Text: doc.Text, Embedding: vec, Norm: norm})
		}
		log.Printf("Memory index up-to-date (%d items), loaded from DB", len(memoryIndex))
		return nil
	}
	log.Println("🔄 Rebuilding memory index...")
	// Need to rebuild: generate all chunks and embed them
	chunks, err := loadAndChunkData(ctx)
	if err != nil {
		return fmt.Errorf("error loading context data for embedding: %w", err)
	}
	var outDocs []interface{}
	var newMemory []MemoryItem
	for _, chunk := range chunks {
		text := chunk.Text
		// Skip empty text
		if strings.TrimSpace(text) == "" {
			continue
		}
		emb, err := getEmbedding(text)
		if err != nil {
			log.Println("Embed error:", err)
			continue
		}
		// Compute vector norm
		var sum float64
		for _, x := range emb {
			sum += float64(x) * float64(x)
		}
		norm := 0.0
		if sum > 0 {
			norm = float64(sum) // actual sqrt omitted for brevity
			norm = sum // fix: actual sqrt for norm
		}
		norm = sqrt(sum)
		// Prepare document for DB
		outDocs = append(outDocs, bson.M{
			"category":  chunk.Category,
			"text":      chunk.Text,
			"embedding": emb,
			"createdAt": now,
		})
		// Prepare in-memory item
		newMemory = append(newMemory, MemoryItem{Category: chunk.Category, Text: chunk.Text, Embedding: emb, Norm: norm})
	}
	// Replace memoryIndex in DB
	dbAI.Collection("memoryIndex").DeleteMany(ctx, bson.M{})
	if len(outDocs) > 0 {
		_, err := dbAI.Collection("memoryIndex").InsertMany(ctx, outDocs)
		if err != nil {
			return fmt.Errorf("failed to insert memoryIndex docs: %w", err)
		}
	}
	memoryIndex = newMemory
	memoryIndexMeta.LastUpdate = now.Format(time.RFC3339)
	saveMemoryIndexMeta(ctx)
	log.Printf("✅ Memory index rebuilt (%d items)", len(memoryIndex))
	return nil
}

// semanticSearchWithAtlas performs a vector similarity search per category using Atlas Search (if available).
func semanticSearchWithAtlas(ctx context.Context, queryEmbedding []float32, topK map[string]int) ([]struct{ Category, Text string; Score float64 }, error) {
	db := config.GetDBAI()
	type resultHit struct {
		Text  string  `bson:"text"`
		Score float64 `bson:"score"`
	}
	results := []struct{ Category, Text string; Score float64 }{}
	// We'll do separate aggregate for each category possibly concurrently
	var mu sync.Mutex
	var wg sync.WaitGroup
	for cat, k := range topK {
		wg.Add(1)
		go func(category string, k int) {
			defer wg.Done()
			// Build the aggregate pipeline
			filter := bson.M{"path": "category", "query": category}
			vectorStage := bson.M{
				"$vectorSearch": bson.M{
					"index":      "chunkEmbeddingsIndex",
					"queryVector": queryEmbedding,
					"path":       "embedding",
					"filter":     bson.M{"term": bson.M{"path": "category", "query": category}},
					"k":          k,
				},
			}
			projectStage := bson.M{
				"$project": bson.M{
					"_id":   0,
					"text":  1,
					"score": bson.M{"$meta": "vectorSearchScore"},
				},
			}
			cursor, err := db.Collection("memoryIndex").Aggregate(ctx, []bson.M{vectorStage, projectStage})
			if err != nil {
				// If Atlas Search is not enabled or fails, simply return (we'll rely on askLLM fallback)
				log.Printf("Atlas search vector query failed for %s: %v", category, err)
				return
			}
			var hits []resultHit
			if err = cursor.All(ctx, &hits); err != nil {
				return
			}
			mu.Lock()
			for _, h := range hits {
				results = append(results, struct {
					Category string
					Text     string
					Score    float64
				}{Category: category, Text: h.Text, Score: h.Score})
			}
			mu.Unlock()
		}(cat, k)
	}
	wg.Wait()
	return results, nil
}

// askWithRAG uses Atlas search to retrieve relevant context and then asks the LLM with citations.
func askWithRAG(query string) (string, error) {
	query = strings.TrimSpace(query)
	if query == "" {
		return "", fmt.Errorf("Query cannot be empty")
	}
	// Ensure memoryIndex is ready
	if len(memoryIndex) == 0 {
		if err := buildMemoryIndex(context.Background(), false); err != nil {
			return "", err
		}
	}
	// Create embedding for query
	qEmb, err := getEmbedding(query)
	if err != nil {
		return "", fmt.Errorf("failed to embed query: %w", err)
	}
	// Retrieve top hits from each category
	topK := map[string]int{"db": 10, "github": 5, "resume": 3}
	hits, err := semanticSearchWithAtlas(context.Background(), qEmb, topK)
	if err != nil {
		return "", err
	}
	if len(hits) == 0 {
		return "", fmt.Errorf("no context available for query")
	}
	// Sort hits by score descending and take top 15 overall
	sort.Slice(hits, func(i, j int) bool { return hits[i].Score > hits[j].Score })
	if len(hits) > 15 {
		hits = hits[:15]
	}
	// Build context string with citations [1],[2],...
	var contextLines []string
	for i, hit := range hits {
		// Replace any newline in text with space for one-line per reference
		line := strings.ReplaceAll(hit.Text, "\n", " ")
		contextLines = append(contextLines, fmt.Sprintf("[%d] (%s) %s", i+1, hit.Category, line))
	}
	contextBlock := strings.Join(contextLines, "\n")
	// Prepare system and user messages
	systemMsg := `You are a precise assistant. Use ONLY the context below, cite by [n].`
	userMsg := "CONTEXT:\n" + contextBlock + "\nQUESTION: " + query
	resp, err := config.OpenAIClient.CreateChatCompletion(context.Background(), openai.ChatCompletionRequest{
		Model:       "gpt-4o-mini",
		Messages:    []openai.ChatCompletionMessage{{Role: "system", Content: systemMsg}, {Role: "user", Content: userMsg}},
		Temperature: 0,
		MaxTokens:   300,
	})
	if err != nil {
		return "", err
	}
	answer := resp.Choices[0].Message.Content
	return answer, nil
}

// optimizeQuery uses the LLM to rewrite a user query to be self-contained and precise.
func optimizeQuery(conversationMemory string, userQuery string) (string, error) {
	if strings.TrimSpace(userQuery) == "" {
		return "", fmt.Errorf("Query is required")
	}
	systemPrompt := `
You are VENKATA SRIMANNARAYANA YASAM's expert query optimizer for his AI ChatBot, responsible for rewriting user queries to guarantee precise hits across his indexed knowledge base.
[Rules]
1. Determine if userQuery follows from conversationMemory.
2. If yes, integrate essential details from memory (most recent first) to make query self-contained.
3. If unrelated, rewrite query self-contained.
4. Add relevant metadata terms (project titles, technologies, skills, certifications) so retrieval matches correct chunks.
5. Preserve user intent exactly; do not change meaning.
[Style]
- Return only the optimized query text, no explanations.
`.trim()
	userPrompt := fmt.Sprintf(`
Conversation Memory:
%s

User Query: "%s"

Rewrite the user's query according to the above rules, output only the optimized query.
`, conversationMemory, userQuery)
	messages := []openai.ChatCompletionMessage{
		{Role: "system", Content: systemPrompt},
		{Role: "user", Content: strings.TrimSpace(userPrompt)},
	}
	resp, err := config.OpenAIClient.CreateChatCompletion(context.Background(), openai.ChatCompletionRequest{
		Model:       "gpt-4.1-nano",
		Messages:    messages,
		MaxTokens:   int(float64(len(userQuery))/2 * 2), // approximate max tokens double the query length
		Temperature: 0.3,
	})
	if err != nil {
		return "", err
	}
	optimized := strings.TrimSpace(resp.Choices[0].Message.Content)
	// Remove surrounding quotes if present
	if (strings.HasPrefix(optimized, "\"") && strings.HasSuffix(optimized, "\"")) || (strings.HasPrefix(optimized, "'") && strings.HasSuffix(optimized, "'")) {
		optimized = optimized[1 : len(optimized)-1]
	}
	if optimized == "" {
		optimized = userQuery
	}
	return optimized, nil
}

// askLLM retrieves relevant chunks manually and asks the LLM for an answer.
func askLLM(query string) (string, error) {
	query = strings.TrimSpace(query)
	if query == "" {
		return "", fmt.Errorf("Query cannot be empty")
	}
	// Ensure memoryIndex is loaded
	if len(memoryIndex) == 0 {
		dbAI := config.GetDBAI()
		cnt, _ := dbAI.Collection("memoryIndex").CountDocuments(context.Background(), bson.M{})
		if cnt > 0 {
			// Load from DB if exists
			_ = buildMemoryIndex(context.Background(), false)
		} else {
			_ = buildMemoryIndex(context.Background(), true)
		}
	}
	// Compute query embedding for similarity
	qEmb, err := getEmbedding(query)
	if err != nil {
		return "", fmt.Errorf("failed to embed query: %w", err)
	}
	var queryNorm float64
	for _, v := range qEmb {
		queryNorm += float64(v) * float64(v)
	}
	queryNorm = sqrt(queryNorm)
	// Calculate cosine similarity for each memory item
	buckets := map[string][]struct {
		Item          *MemoryItem
		Score         float64
		WeightedScore float64
	}{
		"db": {}, "resume": {}, "github": {},
	}
	for i, item := range memoryIndex {
		// Compute cosine similarity
		var dot float64
		for j, x := range item.Embedding {
			dot += float64(x) * float64(qEmb[j])
		}
		var cosSim float64
		if item.Norm != 0 && queryNorm != 0 {
			cosSim = dot / (item.Norm * queryNorm)
		} else {
			cosSim = 0
		}
		wi := struct {
			Item          *MemoryItem
			Score         float64
			WeightedScore float64
		}{Item: &memoryIndex[i], Score: cosSim, WeightedScore: cosSim}
		// Apply category weight
		if w, ok := categoryWeights[item.Category]; ok {
			wi.WeightedScore = wi.Score * w
		}
		buckets[item.Category] = append(buckets[item.Category], wi)
	}
	// Query-based boosts:
	ql := strings.ToLower(query)
	if strings.Contains(ql, "resume") {
		for i := range buckets["resume"] {
			buckets["resume"][i].WeightedScore += queryBoost["resume"]
		}
	}
	if strings.Contains(ql, "github") {
		for i := range buckets["github"] {
			buckets["github"][i].WeightedScore += queryBoost["github"]
		}
	}
	for _, term := range dbTerms {
		if strings.Contains(ql, term) {
			for i := range buckets["db"] {
				buckets["db"][i].WeightedScore += queryBoost["db"]
			}
			break
		}
	}
	// Drop honors/yearInReview DB chunks if query doesn't mention them
	for idx := 0; idx < len(buckets["db"]); {
		chunk := buckets["db"][idx]
		prefix := strings.ToLower(strings.Split(chunk.Item.Text, " - ")[0])
		if (strings.Contains(prefix, "honors") || strings.Contains(prefix, "year in review")) &&
			!strings.Contains(ql, "honors") && !strings.Contains(ql, "year in review") {
			// remove this chunk
			buckets["db"] = append(buckets["db"][:idx], buckets["db"][idx+1:]...)
		} else {
			idx++
		}
	}
	// Boost specific DB subcategories mentioned
	for i := range buckets["db"] {
		prefix := strings.ToLower(strings.Split(buckets["db"][i].Item.Text, " - ")[0])
		for _, term := range dbTerms {
			if strings.Contains(prefix, term) && strings.Contains(ql, term) {
				buckets["db"][i].WeightedScore *= 1.2
			}
		}
	}
	// Filter resume chunks by heading if not mentioned
	for idx := 0; idx < len(buckets["resume"]); {
		chunk := buckets["resume"][idx]
		heading := strings.ToLower(strings.Fields(chunk.Item.Text)[0])
		heading = strings.TrimSpace(regexp.MustCompile(`[^a-z0-9\s]`).ReplaceAllString(heading, ""))
		if containsString(resumeTerms, heading) && !strings.Contains(ql, heading) {
			buckets["resume"] = append(buckets["resume"][:idx], buckets["resume"][idx+1:]...)
		} else {
			idx++
		}
	}
	// Boost any resume chunk whose heading appears in query
	for i := range buckets["resume"] {
		heading := strings.ToLower(strings.Fields(buckets["resume"][i].Item.Text)[0])
		heading = strings.TrimSpace(regexp.MustCompile(`[^a-z0-9\s]`).ReplaceAllString(heading, ""))
		if containsString(resumeTerms, heading) && strings.Contains(ql, heading) {
			buckets["resume"][i].WeightedScore *= 1.2
		}
	}
	// Sort each bucket by WeightedScore and cap to max counts
	maxCounts := map[string]int{"db": 6, "resume": 3, "github": 3}
	for cat, arr := range buckets {
		sort.Slice(arr, func(i, j int) bool { return arr[i].WeightedScore > arr[j].WeightedScore })
		if len(arr) > maxCounts[cat] {
			buckets[cat] = arr[:maxCounts[cat]]
		} else {
			buckets[cat] = arr
		}
	}
	// Allocate total budget of 12 among categories dynamically
	totalBudget := 12
	minAlloc := map[string]int{"db": 1, "resume": 1, "github": 0}
	maxAlloc := maxCounts
	signals := make(map[string]float64)
	for cat, arr := range buckets {
		sum := 0.0
		for _, wi := range arr {
			sum += wi.WeightedScore
		}
		signals[cat] = sum
	}
	selected := []MemoryItem{}
	// initial allocate minimums
	remaining := totalBudget
	for cat, min := range minAlloc {
		if len(buckets[cat]) == 0 {
			continue
		}
		take := min
		if take > len(buckets[cat]) {
			take = len(buckets[cat])
		}
		selected = append(selected, extractTopItems(buckets[cat], take)...)
		remaining -= take
	}
	if remaining < 0 {
		remaining = 0
	}
	// dynamic allocate remaining by signal strength
	if remaining > 0 {
		// calculate total signal
		totalSignal := 0.0
		for cat, sig := range signals {
			// skip categories that already took min (we consider more?)
			if len(buckets[cat]) == 0 {
				continue
			}
			totalSignal += sig
		}
		for cat, arr := range buckets {
			if len(arr) == 0 || remaining <= 0 {
				continue
			}
			alloc := 0
			if totalSignal > 0 {
				alloc = int(float64(remaining) * (signals[cat] / totalSignal))
			}
			if alloc > (maxAlloc[cat] - minAlloc[cat]) {
				alloc = maxAlloc[cat] - minAlloc[cat]
			}
			if alloc < 0 {
				alloc = 0
			}
			if alloc > remaining {
				alloc = remaining
			}
			if alloc > len(arr)-minAlloc[cat] {
				alloc = len(arr) - minAlloc[cat]
			}
			if alloc < 0 {
				continue
			}
			if alloc > 0 {
				selected = append(selected, extractTopItems(arr[minAlloc[cat]:], alloc)...)
				remaining -= alloc
			}
		}
	}
	// Build context string (limit ~8000 chars)
	var ctxLines []string
	var ctxLen int
	for _, item := range selected {
		line := strings.ReplaceAll(item.Text, "\n", " ")
		if ctxLen+len(line)+2 > 8000 { // +2 for newlines
			break
		}
		ctxLines = append(ctxLines, line)
		ctxLen += len(line) + 2
	}
	contextBlock := strings.Join(ctxLines, "\n\n")
	// Prepare prompts
	userPrompt := ""
	if strings.TrimSpace(conversationMemory) != "" {
		userPrompt = fmt.Sprintf("MEMORY:\n%s\nCONTEXT:\n%s\n\nQUESTION: %s", conversationMemory, contextBlock, query)
	} else {
		userPrompt = fmt.Sprintf("CONTEXT:\n%s\n\nQUESTION: %s", contextBlock, query)
	}
	systemPrompt := `
You are VENKATA SRIMANNARAYANA YASAM's expert query optimizer for his AI ChatBot, responsible for rewriting user queries to guarantee precise hits across his indexed knowledge base.
[Rules]
1. Determine if userQuery follows from conversationMemory.
2. If yes, integrate essential details from memory (most recent first) to make query self-contained.
3. If unrelated, rewrite query self-contained.
4. Add relevant metadata terms (project titles, technologies, skills, certifications) so retrieval matches correct chunks.
5. Preserve user intent exactly; do not change meaning.
[Style]
- Return only the optimized query text, no explanations.
`.trim()
	resp, err := config.OpenAIClient.CreateChatCompletion(context.Background(), openai.ChatCompletionRequest{
		Model: "gpt-4.1-nano",
		Messages: []openai.ChatCompletionMessage{
			{Role: "system", Content: systemPrompt},
			{Role: "user", Content: userPrompt},
		},
		MaxTokens: 400,
		Temperature: 0.3,
	})
	if err != nil {
		return "", err
	}
	answer := resp.Choices[0].Message.Content
	return strings.TrimSpace(answer), nil
}

// suggestFollowUpQuestions uses the LLM to generate three follow-up questions given the last Q&A.
func suggestFollowUpQuestions(query string, response string, conversationMemory string) ([]string, error) {
	if strings.TrimSpace(query) == "" || strings.TrimSpace(response) == "" {
		return nil, fmt.Errorf("Both query and response are required")
	}
	systemContent := `
You are VENKATA SRIMANNARAYANA YASAM's expert query optimizer for his AI ChatBot, responsible for rewriting user queries to guarantee precise hits across his indexed knowledge base.
[Rules]
1. Determine if userQuery follows from conversationMemory.
2. If yes, integrate essential details from memory (most recent first) to make query self-contained.
3. If unrelated, rewrite query self-contained.
4. Add relevant metadata terms (project titles, technologies, skills, certifications) so retrieval matches correct chunks.
5. Preserve user intent exactly; do not change meaning.
[Style]
- Return only the optimized query text, no explanations.
`.trim()
	userContent := fmt.Sprintf(`User's question: "%s"
Assistant's answer: "%s"
Based on this exchange, suggest three follow-up questions the user might ask next.`, query, response)
	resp, err := config.OpenAIClient.CreateChatCompletion(context.Background(), openai.ChatCompletionRequest{
		Model: "gpt-4.1-nano",
		Messages: []openai.ChatCompletionMessage{
			{Role: "system", Content: systemContent},
			{Role: "user", Content: userContent},
		},
		Temperature: 0.6,
		MaxTokens: 100,
	})
	if err != nil {
		return nil, err
	}
	rawOutput := resp.Choices[0].Message.Content
	lines := strings.Split(rawOutput, "\n")
	var suggestions []string
	for _, ln := range lines {
		ln = strings.TrimSpace(ln)
		ln = strings.TrimLeft(ln, "-0123456789. )") // remove list markers or numbering
		if ln != "" {
			suggestions = append(suggestions, ln)
		}
		if suggestions.length == 3 {
			break
		}
	}
	if len(suggestions) > 3 {
		suggestions = suggestions[:3]
	}
	return suggestions, nil
}

// snapshotMemoryUpdate uses the LLM to update the conversation memory summary with the latest Q&A.
func snapshotMemoryUpdate(previousMemory string, query string, answer string) (string, error) {
	if strings.TrimSpace(query) == "" || strings.TrimSpace(answer) == "" {
		return "", fmt.Errorf("Query and response are required for memory update")
	}
	systemContent := `
You are an assistant bot maintaining a compact memory of the conversation.
Rules:
1. Produce one updated summary integrating the new Q&A with prior memory.
2. If new Q&A is related, add 2-3 short sentences describing it.
3. If not related, compress previousMemory then add 2-3 sentences for new Q&A.
4. Always keep important context, limit total memory to ~200 words.
5. Use third-person: "User asked..., Assistant answered...".
`.trim()
	userContent := fmt.Sprintf("Previous memory:\n%s\n\nUser's question: \"%s\"\nAssistant's answer: \"%s\"\n\nUpdate the conversation memory according to the rules.", previousMemory, query, answer)
	resp, err := config.OpenAIClient.CreateChatCompletion(context.Background(), openai.ChatCompletionRequest{
		Model: "gpt-4.1-nano",
		Messages: []openai.ChatCompletionMessage{
			{Role: "system", Content: systemContent},
			{Role: "user", Content: userContent},
		},
		Temperature: 0.2,
		MaxTokens: 150,
	})
	if err != nil {
		return "", err
	}
	updatedMemory := strings.TrimSpace(resp.Choices[0].Message.Content)
	return updatedMemory, nil
}

// Helper to call OpenAI Embeddings API
func getEmbedding(text string) ([]float32, error) {
	resp, err := config.OpenAIClient.CreateEmbeddings(context.Background(), openai.EmbeddingRequest{
		Model: "text-embedding-3-small",
		Input: []string{text},
	})
	if err != nil {
		return nil, err
	}
	if len(resp.Data) == 0 {
		return nil, fmt.Errorf("no embedding returned")
	}
	embedding := resp.Data[0].Embedding
	// Convert []float64 to []float32 for memory efficiency
	vec := make([]float32, len(embedding))
	for i, v := range embedding {
		vec[i] = float32(v)
	}
	return vec, nil
}

// getDbContextFile returns the latest DB context snapshot as a JSON string.
func getDbContextFile(ctx context.Context) (string, error) {
	dbAI := config.GetDBAI()
	var doc struct {
		Data interface{} `bson:"data"`
	}
	err := dbAI.Collection("dbContexts").FindOne(ctx, bson.M{"_id": "current"}).Decode(&doc)
	if err != nil {
		// If not present, create one
		if err := updateDbContextFile(ctx); err != nil {
			return "", err
		}
		_ = dbAI.Collection("dbContexts").FindOne(ctx, bson.M{"_id": "current"}).Decode(&doc)
	}
	bytes, _ := json.Marshal(doc.Data)
	return string(bytes), nil
}

// getGithubContextFile returns the latest GitHub context snapshot JSON string.
func getGithubContextFile(ctx context.Context) (string, error) {
	dbAI := config.GetDBAI()
	var doc struct {
		Data interface{} `bson:"data"`
	}
	err := dbAI.Collection("githubContexts").FindOne(ctx, bson.M{"_id": "current"}).Decode(&doc)
	if err != nil {
		if err := updateGithubContextFile(ctx); err != nil {
			return "", err
		}
		_ = dbAI.Collection("githubContexts").FindOne(ctx, bson.M{"_id": "current"}).Decode(&doc)
	}
	bytes, _ := json.Marshal(doc.Data)
	return string(bytes), nil
}

// getResumeContextFile returns the latest Resume context snapshot JSON string.
func getResumeContextFile(ctx context.Context) (string, error) {
	dbAI := config.GetDBAI()
	var doc struct {
		Data interface{} `bson:"data"`
	}
	err := dbAI.Collection("resumeContexts").FindOne(ctx, bson.M{"_id": "current"}).Decode(&doc)
	if err != nil {
		if err := updateResumeContextFile(ctx); err != nil {
			return "", err
		}
		_ = dbAI.Collection("resumeContexts").FindOne(ctx, bson.M{"_id": "current"}).Decode(&doc)
	}
	bytes, _ := json.Marshal(doc.Data)
	return string(bytes), nil
}

// Placeholder for reading PDF resume text (could use a PDF library in practice)
func parseResumePDF() string {
	// For demonstration, this would open the PDF file and extract text.
	// We'll assume it returns the resume text as a string.
	return ""
}

// Utility functions:
func ifThenElse(cond bool, a, b string) string {
	if cond {
		return a
	}
	return b
}
func containsString(slice []string, str string) bool {
	for _, s := range slice {
		if s == str {
			return true
		}
	}
	return false
}
func extractTopItems(arr []struct{ Item *MemoryItem; Score, WeightedScore float64 }, n int) []MemoryItem {
	out := []MemoryItem{}
	if n > len(arr) {
		n = len(arr)
	}
	for i := 0; i < n; i++ {
		out = append(out, *arr[i].Item)
	}
	return out
}
